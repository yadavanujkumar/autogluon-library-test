{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c09a10b",
   "metadata": {},
   "source": [
    "# AutoGluon Tutorial: Automated Machine Learning with Real Datasets\n",
    "\n",
    "## Overview\n",
    "AutoGluon is a powerful AutoML library that enables you to build state-of-the-art machine learning models with just a few lines of code. This tutorial demonstrates:\n",
    "- üöÄ Quick setup and installation\n",
    "- üìä Working with real datasets (including Kaggle)\n",
    "- ü§ñ Training multiple models automatically\n",
    "- üìà Model evaluation and comparison\n",
    "- üéØ Making predictions\n",
    "\n",
    "**Author**: AutoGluon Tutorial  \n",
    "**Dataset**: Titanic (Kaggle)  \n",
    "**Task**: Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5d197",
   "metadata": {},
   "source": [
    "## 1. Install and Import AutoGluon\n",
    "\n",
    "First, we'll install AutoGluon and import the necessary libraries. AutoGluon provides specialized modules for different data types (tabular, text, image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AutoGluon (uncomment if not already installed)\n",
    "# !pip install autogluon\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")\n",
    "print(f\"AutoGluon version: {TabularPredictor.__module__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70617dd6",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from Kaggle\n",
    "\n",
    "We'll use the famous **Titanic dataset** for this tutorial. This dataset contains information about Titanic passengers and whether they survived.\n",
    "\n",
    "### Option A: Load from Seaborn (Built-in)\n",
    "For quick start, we'll use seaborn's built-in Titanic dataset.\n",
    "\n",
    "### Option B: Load from Kaggle API\n",
    "To use Kaggle datasets:\n",
    "1. Get Kaggle API token from kaggle.com/account\n",
    "2. Place kaggle.json in ~/.kaggle/\n",
    "3. Use: `!kaggle datasets download -d kaggle/titanic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e02366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load from seaborn (built-in dataset)\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "print(\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load from Kaggle (uncomment if you have Kaggle API configured)\n",
    "# !kaggle datasets download -d kaggle/titanic -p ./data --unzip\n",
    "# df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d19911",
   "metadata": {},
   "source": [
    "## 3. Explore the Dataset\n",
    "\n",
    "Let's explore the dataset to understand its structure, features, and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317286e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"Target Variable (Survived) Distribution:\")\n",
    "print(df['survived'].value_counts())\n",
    "print(f\"\\nSurvival Rate: {df['survived'].mean():.2%}\")\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['survived'].value_counts().plot(kind='bar', color=['#e74c3c', '#2ecc71'])\n",
    "plt.title('Survival Count')\n",
    "plt.xlabel('Survived (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['survived'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['#e74c3c', '#2ecc71'])\n",
    "plt.title('Survival Distribution')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becde519",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training\n",
    "\n",
    "We'll select relevant features and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for modeling\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'survived']\n",
    "df_model = df[features].copy()\n",
    "\n",
    "# Drop rows with missing target variable\n",
    "df_model = df_model.dropna(subset=['survived'])\n",
    "\n",
    "print(f\"Dataset shape after preprocessing: {df_model.shape}\")\n",
    "print(f\"\\nSelected features: {[col for col in df_model.columns if col != 'survived']}\")\n",
    "print(f\"Target variable: survived\")\n",
    "\n",
    "# Display a sample\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    df_model, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df_model['survived']  # Maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data split successfully!\")\n",
    "print(f\"\\nTraining set size: {len(train_data)} samples\")\n",
    "print(f\"Test set size: {len(test_data)} samples\")\n",
    "print(f\"\\nTraining set survival rate: {train_data['survived'].mean():.2%}\")\n",
    "print(f\"Test set survival rate: {test_data['survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033238f",
   "metadata": {},
   "source": [
    "## 5. Train AutoGluon Model\n",
    "\n",
    "Now comes the magic! AutoGluon will automatically:\n",
    "- Handle missing values\n",
    "- Encode categorical variables\n",
    "- Train multiple models (Random Forest, XGBoost, Neural Networks, etc.)\n",
    "- Perform hyperparameter tuning\n",
    "- Create ensemble models\n",
    "- Select the best model\n",
    "\n",
    "**All with just 3 lines of code!** üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8758b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor\n",
    "predictor = TabularPredictor(\n",
    "    label='survived',           # Target column\n",
    "    problem_type='binary',      # Can be 'binary', 'multiclass', 'regression'\n",
    "    eval_metric='accuracy',     # Metric to optimize\n",
    "    path='./ag_models/titanic'  # Where to save models\n",
    ")\n",
    "\n",
    "print(\"‚úì Predictor initialized!\")\n",
    "print(f\"\\nProblem Type: {predictor.problem_type}\")\n",
    "print(f\"Evaluation Metric: accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c81fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# This will train multiple models and create ensembles\n",
    "# Training time can be adjusted based on your needs\n",
    "\n",
    "predictor.fit(\n",
    "    train_data=train_data,\n",
    "    time_limit=120,              # Time limit in seconds (2 minutes)\n",
    "    presets='medium_quality',    # Options: 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n",
    "    verbosity=2                  # 0=silent, 1=minimal, 2=normal, 3=detailed\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Training completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71743c8",
   "metadata": {},
   "source": [
    "### Understanding Training Parameters\n",
    "\n",
    "- **time_limit**: Total time budget for training (in seconds). More time = better models\n",
    "- **presets**: Quality/speed trade-off\n",
    "  - `best_quality`: Highest accuracy, slowest (competition setting)\n",
    "  - `high_quality`: High accuracy, slower\n",
    "  - `good_quality`: Good accuracy, moderate speed\n",
    "  - `medium_quality`: Decent accuracy, faster (prototyping)\n",
    "- **verbosity**: Amount of output information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b7be",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Let's evaluate how well our model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe36e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data, silent=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test Accuracy: {performance:.4f} ({performance*100:.2f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be79dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Leaderboard - Compare all trained models\n",
    "leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "\n",
    "print(\"\\nüìä Model Leaderboard (All Trained Models):\")\n",
    "print(\"=\"*80)\n",
    "print(leaderboard)\n",
    "print(\"\\nüí° The model at the top performed best on the test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get top 10 models\n",
    "top_models = leaderboard.head(10)\n",
    "\n",
    "plt.barh(range(len(top_models)), top_models['score_test'], color='skyblue')\n",
    "plt.yticks(range(len(top_models)), top_models['model'])\n",
    "plt.xlabel('Test Score (Accuracy)')\n",
    "plt.title('Top 10 Model Performance Comparison')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed performance metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions\n",
    "y_pred = predictor.predict(test_data.drop(columns=['survived']))\n",
    "y_true = test_data['survived']\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìà Detailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=['Did not survive', 'Survived']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Did not survive', 'Survived'],\n",
    "            yticklabels=['Did not survive', 'Survived'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Confusion Matrix Interpretation:\")\n",
    "print(f\"   True Negatives (TN): {cm[0,0]} - Correctly predicted did not survive\")\n",
    "print(f\"   False Positives (FP): {cm[0,1]} - Incorrectly predicted survived\")\n",
    "print(f\"   False Negatives (FN): {cm[1,0]} - Incorrectly predicted did not survive\")\n",
    "print(f\"   True Positives (TP): {cm[1,1]} - Correctly predicted survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd2bdc",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "\n",
    "Now let's use our trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample passengers for prediction\n",
    "sample_passengers = pd.DataFrame({\n",
    "    'pclass': [3, 1, 2, 3, 1],\n",
    "    'sex': ['male', 'female', 'male', 'female', 'male'],\n",
    "    'age': [22, 38, 26, 35, 54],\n",
    "    'sibsp': [1, 1, 0, 0, 0],\n",
    "    'parch': [0, 0, 0, 2, 1],\n",
    "    'fare': [7.25, 71.28, 13.0, 20.5, 51.86],\n",
    "    'embarked': ['S', 'C', 'S', 'S', 'S']\n",
    "})\n",
    "\n",
    "print(\"Sample Passengers for Prediction:\")\n",
    "print(sample_passengers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e152af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = predictor.predict(sample_passengers)\n",
    "probabilities = predictor.predict_proba(sample_passengers)\n",
    "\n",
    "print(\"\\nüéØ Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(len(sample_passengers)):\n",
    "    passenger = sample_passengers.iloc[i]\n",
    "    pred = predictions.iloc[i]\n",
    "    prob = probabilities.iloc[i]\n",
    "    \n",
    "    print(f\"\\nPassenger {i+1}:\")\n",
    "    print(f\"  Class: {passenger['pclass']}, Sex: {passenger['sex']}, Age: {passenger['age']}\")\n",
    "    print(f\"  Fare: ${passenger['fare']:.2f}\")\n",
    "    print(f\"  üîÆ Prediction: {'‚úì SURVIVED' if pred == 1 else '‚úó DID NOT SURVIVE'}\")\n",
    "    print(f\"  üìä Confidence: {max(prob):.2%}\")\n",
    "    print(f\"  üìà Probabilities: Did not survive: {prob[0]:.2%}, Survived: {prob[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024dddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "passenger_labels = [f\"P{i+1}\\n{row['sex'][0].upper()}, {row['age']}y\\nClass {row['pclass']}\" \n",
    "                   for i, row in sample_passengers.iterrows()]\n",
    "\n",
    "# Plot survival probabilities\n",
    "x = np.arange(len(sample_passengers))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, probabilities.iloc[:, 0], width, label='Did Not Survive', color='#e74c3c')\n",
    "bars2 = ax.bar(x + width/2, probabilities.iloc[:, 1], width, label='Survived', color='#2ecc71')\n",
    "\n",
    "ax.set_xlabel('Passenger')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Survival Prediction Probabilities for Sample Passengers')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(passenger_labels)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6c25f",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "Understanding which features are most important for predictions helps us:\n",
    "- Gain insights into the problem\n",
    "- Identify key factors\n",
    "- Improve data collection\n",
    "- Build trust in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = predictor.feature_importance(test_data)\n",
    "\n",
    "print(\"üìä Feature Importance:\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance)\n",
    "print(\"\\nüí° Higher values = more important for predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64107a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))\n",
    "feature_importance.plot(kind='barh', color=colors)\n",
    "\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance for Titanic Survival Prediction', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nüîç Feature Importance Interpretation:\")\n",
    "print(\"-\" * 60)\n",
    "top_feature = feature_importance.idxmax()\n",
    "print(f\"Most important feature: '{top_feature}'\")\n",
    "print(f\"\\nThis means '{top_feature}' has the strongest influence on survival predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885f9c9",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "‚úÖ Loaded a real dataset (Titanic from Kaggle)  \n",
    "‚úÖ Explored and preprocessed the data  \n",
    "‚úÖ Trained multiple ML models automatically  \n",
    "‚úÖ Evaluated model performance  \n",
    "‚úÖ Made predictions on new data  \n",
    "‚úÖ Analyzed feature importance  \n",
    "\n",
    "### Key Takeaways\n",
    "1. **AutoGluon is powerful**: State-of-art results with minimal code\n",
    "2. **Automatic everything**: Preprocessing, model selection, hyperparameter tuning\n",
    "3. **Multiple models**: Trains and compares many models automatically\n",
    "4. **Easy to use**: Perfect for beginners and experts alike\n",
    "\n",
    "### Next Steps to Try\n",
    "\n",
    "1. **Use Different Datasets**:\n",
    "   - Download other Kaggle datasets\n",
    "   - Try regression problems (house prices, stock prediction)\n",
    "   - Work with larger datasets\n",
    "\n",
    "2. **Customize Training**:\n",
    "   ```python\n",
    "   # Increase training time for better results\n",
    "   predictor.fit(train_data, time_limit=600, presets='best_quality')\n",
    "   \n",
    "   # Specify which models to use\n",
    "   predictor.fit(train_data, hyperparameters={\n",
    "       'GBM': {},     # LightGBM\n",
    "       'XGB': {},     # XGBoost\n",
    "       'CAT': {},     # CatBoost\n",
    "       'RF': {}       # Random Forest\n",
    "   })\n",
    "   ```\n",
    "\n",
    "3. **Advanced Features**:\n",
    "   - Multi-label classification\n",
    "   - Time series forecasting\n",
    "   - Text and image data\n",
    "   - Custom feature engineering\n",
    "\n",
    "### Resources\n",
    "- üìñ [AutoGluon Documentation](https://auto.gluon.ai/)\n",
    "- üíª [GitHub Repository](https://github.com/autogluon/autogluon)\n",
    "- üéì [Tutorials](https://auto.gluon.ai/stable/tutorials/index.html)\n",
    "- üèÜ [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy AutoML! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
